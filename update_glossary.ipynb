{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages/distutils-precedence.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\n",
      "\n",
      "Remainder of file ignored\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: importlib-metadata in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from transformers) (5.0.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: requests in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from transformers) (3.8.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from requests->transformers) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from requests->transformers) (1.25.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "\u001b[33mWARNING: Error parsing requirements for tensorflow-estimator: [Errno 2] No such file or directory: '/home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages/tensorflow_estimator-2.1.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n",
      "Error processing line 1 of /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages/distutils-precedence.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\n",
      "\n",
      "Remainder of file ignored\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ntlk (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for ntlk\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages/distutils-precedence.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\n",
      "\n",
      "Remainder of file ignored\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m515.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Requirement already satisfied: typing-extensions in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Requirement already satisfied: setuptools in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (47.1.1)\n",
      "Requirement already satisfied: wheel in /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for tensorflow-estimator: [Errno 2] No such file or directory: '/home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages/tensorflow_estimator-2.1.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages/distutils-precedence.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\n",
      "\n",
      "Remainder of file ignored\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "import itertools\n",
    "from typing import Union, List, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "import difflib\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import Levenshtein\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import json \n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import nltk\n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/c/Users/marti/Downloads/englishwords.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "words = data.keys()\n",
    "words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode_dict:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_word = False\n",
    "        self.description = None\n",
    "\n",
    "class Trie_dict:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode_dict()\n",
    "    \n",
    "    def insert(self, word, description = None):\n",
    "        current = self.root\n",
    "        for char in word:\n",
    "            if char not in current.children:\n",
    "                current.children[char] = TrieNode_dict()\n",
    "            current = current.children[char]\n",
    "        current.is_word = True\n",
    "        current.description = description\n",
    "    \n",
    "    def search(self, word):\n",
    "        current = self.root\n",
    "        for char in word:\n",
    "            if char not in current.children:\n",
    "                return None\n",
    "            current = current.children[char]\n",
    "        if current.is_word:\n",
    "            return current.description\n",
    "        return None\n",
    "\n",
    "    def insert_list(self, lst):\n",
    "        for word in lst:\n",
    "            self.insert(word) \n",
    "    \n",
    "    def fuzzy_search(self, word, cutoff=0.6):\n",
    "        results = difflib.get_close_matches(word, self.words(), n=10, cutoff=cutoff)\n",
    "        return {result: (self.search(result), difflib.SequenceMatcher(None, word, result).ratio()) for result in results}\n",
    "\n",
    "    def fuzzy_search_knn(self, word, k=5, cutoff=0.4):\n",
    "        words = np.array(self.words())\n",
    "        words_len = np.array([len(w) for w in words])\n",
    "        word_len = len(word)\n",
    "        distances = np.abs(words_len - word_len)\n",
    "        knn = NearestNeighbors(n_neighbors=k, metric='manhattan')\n",
    "        knn.fit(distances.reshape(-1, 1))\n",
    "        _, indices = knn.kneighbors(np.array([word_len]).reshape(-1, 1))\n",
    "        results = [words[index] for index in indices[0]]\n",
    "        ratio = [difflib.SequenceMatcher(None, word, result).ratio() for result in results]\n",
    "        return {result: (self.search(result), ratio[i]) for i, result in enumerate(results) if ratio[i] >= cutoff}\n",
    "        \n",
    "        \n",
    "    def words(self):\n",
    "        words = []\n",
    "        def dfs(node, word):\n",
    "            if node.is_word:\n",
    "                words.append(word)\n",
    "            for char in node.children:\n",
    "                dfs(node.children[char], word + char)\n",
    "        dfs(self.root, \"\")\n",
    "        return words\n",
    "\n",
    "\n",
    "def extract_description(trie, key):\n",
    "    return trie.search(key)\n",
    "\n",
    "trie_dict = Trie_dict()\n",
    "\n",
    "trie_dict.insert_list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/gmartini2019/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gmartini2019/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/gmartini2019/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/gmartini2019/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gmartini2019/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import wordcloud\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "synsets = wordnet.synsets('')\n",
    "for synset in synsets:\n",
    "    print(synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "# Define a function to fine-tune the BERT model\n",
    "def fine_tune(word, definition):\n",
    "    if definition is None:\n",
    "        label = torch.tensor([[0.0]])\n",
    "    else:\n",
    "        label = torch.tensor([[1.0]])\n",
    "\n",
    "    print(f\"Fine-tuning the model with '{word}': '{definition}'\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        inputs = tokenizer(word, definition, return_tensors='pt', padding=True, truncation=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs, labels=torch.tensor([[1.0]]))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Check if the definition of the word is in WordNet\n",
    "def get_definition(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if synsets:\n",
    "        return synsets[0].definition()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def predict_definition(word, description):\n",
    "    inputs = tokenizer(word, description, return_tensors='pt', padding=True, truncation=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = outputs.logits.sigmoid()\n",
    "    words = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    prediction_indices = torch.where(probabilities > 0.1)[1]\n",
    "    prediction = [words[i] for i in prediction_indices]\n",
    "    #definition = ' '.join(prediction) or \"Sorry, I could not predict the definition of this word.\"\n",
    "    return definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_definition(word):\n",
    "  # Ask the user for a word\n",
    "\n",
    "  # Check if the definition of the word is in the BERT model's knowledge base\n",
    "  definition = get_definition(word)\n",
    "  if definition:\n",
    "      print(f\"The definition of '{word}' is: {definition}\")\n",
    "      trie_dict.insert(word, definition)\n",
    "  else:\n",
    "      print(f\"Sorry, I don't know the definition of '{word}'.\")\n",
    "      definition = input(f\"Please enter the definition of '{word}': \")\n",
    "      fine_tune(word, definition)\n",
    "      print(f\"Thanks, I've learned the definition of '{word}'.\")\n",
    "      predicted_definition = predict_definition(word, definition)\n",
    "      new_string = \"\".join(predicted_definition)\n",
    "      new_string = new_string.replace('#', '')\n",
    "      new_string = new_string.replace('[CLS]', '')\n",
    "      new_string = new_string.replace('[SEP]', ' ')\n",
    "      print(f\"The predicted definition of '{word}' is: {new_string}\")\n",
    "      print(type(predicted_definition))\n",
    "      trie_dict.insert(word, new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I don't know the definition of 'fname'.\n",
      "Fine-tuning the model with 'fname': 'First name, usually'\n",
      "Thanks, I've learned the definition of 'fname'.\n",
      "The predicted definition of 'fname' is: \n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['currency', 'current', 'recurrency', 'recurrence', 'occurrence', 'incurrence', 'decurrency', 'decurrence', 'currencies', 'currents']\n"
     ]
    }
   ],
   "source": [
    "search = input('What would you like to search?')\n",
    "search_result = list(trie_dict.fuzzy_search(search))\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the term for which you would like the definition\n",
      "1. love\n",
      "2. lovey\n",
      "3. loves\n",
      "4. lover\n",
      "5. lovee\n",
      "6. loved\n",
      "7. loave\n",
      "8. glove\n",
      "9. clove\n",
      "10. loe\n",
      "If the desired item is in the list, type Y\n",
      "Now type the number associated to the desired term\n",
      "You selected: glove\n",
      "No previous definition has been found, however glove is commonly referred to as:\n",
      " \n",
      "the handwear used by fielders in playing baseball\n",
      "Now you can define it yourself\n",
      "Thank you, I learned a new word!\n"
     ]
    }
   ],
   "source": [
    "def navigate_trie(trie_dict):\n",
    "    # Prompt the user to enter a search term\n",
    "    print('Enter the term for which you would like the definition')\n",
    "    search = input()\n",
    "    \n",
    "    # Use the trie data structure to perform a fuzzy search on the search term\n",
    "    search_result = list(trie_dict.fuzzy_search(search))\n",
    "    list_length = len(search_result)\n",
    "    \n",
    "    # If the search term is not found in the trie, prompt the user to define it\n",
    "    if list_length == 0:\n",
    "        print('It is not in our list, please define it yourself')\n",
    "        definition = input()\n",
    "        fine_tune(search, definition)\n",
    "        print(f\"Thanks, I've learned the definition of '{search}'.\")\n",
    "        \n",
    "    # If the search term is found in the trie, present the user with a list of search results\n",
    "    else:\n",
    "        for i, item in enumerate(search_result):\n",
    "            print(f\"{i+1}. {item}\")\n",
    "        \n",
    "        # Prompt the user to select a search result from the list\n",
    "        print('If the desired item is in the list, type Y')\n",
    "        choice = input()\n",
    "        if choice == 'Y':\n",
    "            print('Now type the number associated to the desired term')\n",
    "            user_choice = int(input())\n",
    "            \n",
    "            # Ensure that the user's choice is a valid index in the list of search results\n",
    "            1 <= user_choice <= len(search_result)\n",
    "            selected_item = search_result[user_choice - 1]\n",
    "            print(f\"You selected: {selected_item}\")\n",
    "            \n",
    "            # Look up the definition of the selected term in the trie data structure\n",
    "            word_definition = trie_dict.search(selected_item)\n",
    "            \n",
    "            # If the term has no definition in the trie, prompt the user to define it\n",
    "            if word_definition == None:\n",
    "                print(f'No previous definition has been found, however {selected_item} is commonly referred to as:\\n ')\n",
    "                print(get_definition(selected_item))\n",
    "                print('Now you can define it yourself')\n",
    "                custom_definition = input()\n",
    "                trie_dict.insert(selected_item, custom_definition)\n",
    "                print('Thank you, I learned a new word!')\n",
    "            \n",
    "            # If the term has a definition in the trie, present the definition to the user and prompt for redefinition\n",
    "            else:\n",
    "                print(f'The definition for {selected_item} is : {word_definition}')\n",
    "                print(f'Do you like it? Type \"Y\" if so, if not you\"ll redefine it')\n",
    "                redefinition_choice = input()\n",
    "                \n",
    "                # If the user chooses to redefine the term, prompt for a new definition and update the trie\n",
    "                if redefinition_choice != 'Y':\n",
    "                    print('Type it in:')\n",
    "                    custom_definition = input()\n",
    "                    trie_dict.insert(selected_item, custom_definition)\n",
    "                    print('Thank you, I learned a new word!')\n",
    "navigate_trie(trie_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dictionary(dict):\n",
    " \n",
    "  # __init__ function\n",
    "  def __init__(self):\n",
    "    self = dict()\n",
    " \n",
    "  # Function to add key:value\n",
    "  def add(self, key, value):\n",
    "    self[key] = value\n",
    " \n",
    " \n",
    "# Main Function\n",
    "dict_obj = my_dictionary()\n",
    "\n",
    "def get_definition(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if synsets:\n",
    "        return synsets[0].definition()\n",
    "    else:\n",
    "        return None\n",
    "for i in range(len(words)):\n",
    "  definition = get_definition(words[i])\n",
    "  dict_obj.add(words[i], definition)\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_terms_ds = pd.DataFrame(columns = [['Term', 'Definition']])\n",
    "df = pd.DataFrame(list(dict_obj.items()), columns=['Terms', 'Definitions'])\n",
    "df.to_csv('wordnet_english_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms</th>\n",
       "      <th>Definitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a metric unit of length equal to one ten billi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>a dry form of lava resembling clinkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaa</td>\n",
       "      <td>an aneurysm of the abdominal aorta associated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aah</td>\n",
       "      <td>express admiration and pleasure by uttering `o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aahed</td>\n",
       "      <td>express admiration and pleasure by uttering `o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370081</th>\n",
       "      <td>zuni</td>\n",
       "      <td>a member of the Pueblo people living in wester...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370084</th>\n",
       "      <td>zunis</td>\n",
       "      <td>a member of the Pueblo people living in wester...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370086</th>\n",
       "      <td>zurich</td>\n",
       "      <td>the largest city in Switzerland; located in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370092</th>\n",
       "      <td>zwieback</td>\n",
       "      <td>slice of sweet raised bread baked again until ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370093</th>\n",
       "      <td>zwiebacks</td>\n",
       "      <td>slice of sweet raised bread baked again until ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112272 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Terms                                        Definitions\n",
       "0               a  a metric unit of length equal to one ten billi...\n",
       "1              aa             a dry form of lava resembling clinkers\n",
       "2             aaa  an aneurysm of the abdominal aorta associated ...\n",
       "3             aah  express admiration and pleasure by uttering `o...\n",
       "4           aahed  express admiration and pleasure by uttering `o...\n",
       "...           ...                                                ...\n",
       "370081       zuni  a member of the Pueblo people living in wester...\n",
       "370084      zunis  a member of the Pueblo people living in wester...\n",
       "370086     zurich  the largest city in Switzerland; located in th...\n",
       "370092   zwieback  slice of sweet raised bread baked again until ...\n",
       "370093  zwiebacks  slice of sweet raised bread baked again until ...\n",
       "\n",
       "[112272 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying another BERT to train it on the English dictionary again but failing miserably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_dataloader(dataset, batch_size=32):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages/distutils-precedence.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\n",
      "\n",
      "Remainder of file ignored\n",
      "Collecting bert\n",
      "  Downloading bert-2.2.0.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting erlastic\n",
      "  Downloading erlastic-2.0.0.tar.gz (6.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: bert, erlastic\n",
      "  Building wheel for bert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bert: filename=bert-2.2.0-py3-none-any.whl size=3754 sha256=60792f747e259ab7b9e5403d82f2885ad6bc2af3ce17f660ebb9171807de571c\n",
      "  Stored in directory: /home/gmartini2019/.cache/pip/wheels/bb/31/1b/c05f362e347429b7436954d1a2280fe464731e8f569123a848\n",
      "  Building wheel for erlastic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for erlastic: filename=erlastic-2.0.0-py3-none-any.whl size=6787 sha256=489cc027596e155a3b1ef4a8cdc839f9eb30a181494c6cc3ba74bd2a59f3522b\n",
      "  Stored in directory: /home/gmartini2019/.cache/pip/wheels/94/f1/b4/0b98b1e94775da6a0b1130e342d22af05cd269e1172c19f40f\n",
      "Successfully built bert erlastic\n",
      "\u001b[33mWARNING: Error parsing requirements for tensorflow-estimator: [Errno 2] No such file or directory: '/home/gmartini2019/miniconda3/envs/bruce_environment/lib/python3.7/site-packages/tensorflow_estimator-2.1.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: erlastic, bert\n",
      "Successfully installed bert-2.2.0 erlastic-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2560/1354773098.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Convert the tokenized data into PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the DataFrame\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['Terms'].values, df['Definitions'].values, test_size=0.2)\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
    "\n",
    "# Convert the tokenized data into PyTorch tensors\n",
    "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_labels))\n",
    "val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_labels))\n",
    "\n",
    "# Define the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, inputs['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1} training loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += inputs['labels'].size(0)\n",
    "        correct += (predicted == inputs['labels']).sum().item()\n",
    "print(f'Accuracy on validation set: {100 * correct / total}%')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'bert_model.pt')\n",
    "\n",
    "# Use the model to make a prediction\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.load_state_dict(torch.load('bert_model.pt'))\n",
    "\n",
    "input_value = \"example input\"\n",
    "input_encoding = tokenizer(input_value, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = {'input_ids': input_encoding['input_ids'], 'attention_mask': input_encoding['attention_mask']}\n",
    "    outputs = model(**inputs)\n",
    "    _, predicted = torch.max(outputs.logits, 1)\n",
    "    predicted_output = predicted.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
    "\n",
    "train_encodings = np.array(train_encodings)\n",
    "train_encodings = train_encodings.astype(int)\n",
    "print(type(train_encodings))\n",
    "\n",
    "# Convert the tokenized data into PyTorch tensors\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_labels))\n",
    "val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_labels))\n",
    "\n",
    "# Define the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, inputs['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1} training loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += inputs['labels'].size(0)\n",
    "        correct += (predicted == inputs['labels']).sum().item()\n",
    "print(f'Accuracy on validation set: {100 * correct / total}%')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'bert_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bruce_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9b4a2bb493eda419a542eae9e195fcc22a0fd830f7d7e5d4eaaa6ec7ed188b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
