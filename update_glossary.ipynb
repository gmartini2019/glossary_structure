{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "import itertools\n",
    "from typing import Union, List, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "import difflib\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import Levenshtein\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import json \n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import nltk\n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/Colab Notebooks/englishwords.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "words = data.keys()\n",
    "words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode_dict:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_word = False\n",
    "        self.description = None\n",
    "\n",
    "class Trie_dict:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode_dict()\n",
    "    \n",
    "    def insert(self, word, description = None):\n",
    "        current = self.root\n",
    "        for char in word:\n",
    "            if char not in current.children:\n",
    "                current.children[char] = TrieNode_dict()\n",
    "            current = current.children[char]\n",
    "        current.is_word = True\n",
    "        current.description = description\n",
    "    \n",
    "    def search(self, word):\n",
    "        current = self.root\n",
    "        for char in word:\n",
    "            if char not in current.children:\n",
    "                return None\n",
    "            current = current.children[char]\n",
    "        if current.is_word:\n",
    "            return current.description\n",
    "        return None\n",
    "\n",
    "    def insert_list(self, lst):\n",
    "        for word in lst:\n",
    "            self.insert(word) \n",
    "    \n",
    "    def fuzzy_search(self, word, cutoff=0.6):\n",
    "        results = difflib.get_close_matches(word, self.words(), n=10, cutoff=cutoff)\n",
    "        return {result: (self.search(result), difflib.SequenceMatcher(None, word, result).ratio()) for result in results}\n",
    "\n",
    "    def fuzzy_search_knn(self, word, k=5, cutoff=0.4):\n",
    "        words = np.array(self.words())\n",
    "        words_len = np.array([len(w) for w in words])\n",
    "        word_len = len(word)\n",
    "        distances = np.abs(words_len - word_len)\n",
    "        knn = NearestNeighbors(n_neighbors=k, metric='manhattan')\n",
    "        knn.fit(distances.reshape(-1, 1))\n",
    "        _, indices = knn.kneighbors(np.array([word_len]).reshape(-1, 1))\n",
    "        results = [words[index] for index in indices[0]]\n",
    "        ratio = [difflib.SequenceMatcher(None, word, result).ratio() for result in results]\n",
    "        return {result: (self.search(result), ratio[i]) for i, result in enumerate(results) if ratio[i] >= cutoff}\n",
    "        \n",
    "        \n",
    "    def words(self):\n",
    "        words = []\n",
    "        def dfs(node, word):\n",
    "            if node.is_word:\n",
    "                words.append(word)\n",
    "            for char in node.children:\n",
    "                dfs(node.children[char], word + char)\n",
    "        dfs(self.root, \"\")\n",
    "        return words\n",
    "\n",
    "\n",
    "def extract_description(trie, key):\n",
    "    return trie.search(key)\n",
    "\n",
    "trie_dict = Trie_dict()\n",
    "\n",
    "trie_dict.insert_list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "synsets = wordnet.synsets('')\n",
    "for synset in synsets:\n",
    "    print(synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "# Define a function to fine-tune the BERT model\n",
    "def fine_tune(word, definition):\n",
    "    if definition is None:\n",
    "        label = torch.tensor([[0.0]])\n",
    "    else:\n",
    "        label = torch.tensor([[1.0]])\n",
    "\n",
    "    print(f\"Fine-tuning the model with '{word}': '{definition}'\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        inputs = tokenizer(word, definition, return_tensors='pt', padding=True, truncation=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs, labels=torch.tensor([[1.0]]))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Check if the definition of the word is in WordNet\n",
    "def get_definition(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if synsets:\n",
    "        return synsets[0].definition()\n",
    "    else:\n",
    "        return \"None\"\n",
    "\n",
    "def predict_definition(word, description):\n",
    "    inputs = tokenizer(word, description, return_tensors='pt', padding=True, truncation=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = outputs.logits.sigmoid()\n",
    "    words = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    prediction_indices = torch.where(probabilities > 0.1)[1]\n",
    "    prediction = [words[i] for i in prediction_indices]\n",
    "    #definition = ' '.join(prediction) or \"Sorry, I could not predict the definition of this word.\"\n",
    "    return definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_definition():\n",
    "  # Ask the user for a word\n",
    "  word = input(\"Enter a word: \")\n",
    "\n",
    "  # Check if the definition of the word is in the BERT model's knowledge base\n",
    "  definition = get_definition(word)\n",
    "  if definition:\n",
    "      print(f\"The definition of '{word}' is: {definition}\")\n",
    "      trie_dict.insert(word, definition)\n",
    "  else:\n",
    "      print(f\"Sorry, I don't know the definition of '{word}'.\")\n",
    "      definition = input(f\"Please enter the definition of '{word}': \")\n",
    "      fine_tune(word, definition)\n",
    "      print(f\"Thanks, I've learned the definition of '{word}'.\")\n",
    "      predicted_definition = predict_definition(word, definition)\n",
    "      new_string = \"\".join(predicted_definition)\n",
    "      new_string = new_string.replace('#', '')\n",
    "      new_string = new_string.replace('[CLS]', '')\n",
    "      new_string = new_string.replace('[SEP]', ' ')\n",
    "      print(f\"The predicted definition of '{word}' is: {new_string}\")\n",
    "      print(type(predicted_definition))\n",
    "      trie_dict.insert(word, new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = input('What would you like to search?')\n",
    "search_result = list(trie_dict.fuzzy_search(search))\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_trie(trie_dict):  \n",
    "  search = input('What would you like to search?')\n",
    "  search_result = list(trie_dict.fuzzy_search(search))\n",
    "  \n",
    "  if search_result == \"\":\n",
    "    print('Not in our list')\n",
    "    print('Define it and att it to the model!!')\n",
    "    dl_definition()\n",
    "  else:\n",
    "    print(search_result)\n",
    "    print('Is the desired term in the list?')\n",
    "    isit = input('[yes/no]')\n",
    "    if isit == 'yes':\n",
    "        print(\"Please select an item from the list:\")\n",
    "        for i, item in enumerate(search_result):\n",
    "            print(f\"{i+1}. {item}\")\n",
    "        while True:\n",
    "            try:\n",
    "                user_choice = int(input(\"Enter the number corresponding to your choice: \"))\n",
    "                if 1 <= user_choice <= len(search_result):\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Invalid choice. Please enter a number between 1 and {len(search_result)}\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "        selected_item = search_result[user_choice - 1]\n",
    "        print(f\"You selected: {selected_item}\")\n",
    "        url = f\"https://api.dictionaryapi.dev/api/v2/entries/en/{selected_item}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "          definition = response.json()[0][\"meanings\"][0][\"definitions\"][0][\"definition\"]\n",
    "          custom_def = trie_dict.search(selected_item)\n",
    "\n",
    "          if custom_def == None:\n",
    "\n",
    "            print(f'No previous definition has been found, however {selected_item} is commonly referred to as:\\n ')\n",
    "            print(f\"Definition : {definition}\")\n",
    "            print('Is this definition satisfactory? If it is, type \"Y\", otherwise type anything else')\n",
    "            choice_description = input('Choice: ')\n",
    "            if choice_description == 'Y':\n",
    "              trie_dict.insert(selected_item, definition)\n",
    "            else:\n",
    "              definition = input('Then insert your defintion: ')\n",
    "              trie_dict.insert(selected_item, definition)\n",
    "          \n",
    "          else:\n",
    "            print(custom_def)\n",
    "            print('Is this definition satisfactory? If it is, type \"Y\", otherwise type anything else')\n",
    "            choice_description = input('Choice: ')\n",
    "            if choice_description == 'Y':\n",
    "              trie_dict.insert(selected_item, custom_def)\n",
    "            else:\n",
    "              custom_def = input('Then insert your defintion: ')\n",
    "              trie_dict.insert(selected_item, custom_def)\n",
    "              \n",
    "        except KeyError:\n",
    "            print('Definition not found, would you like to add it? Type \"Y\" if so')\n",
    "            choice_description = input('Choice: ')\n",
    "            if choice_description == 'Y':\n",
    "              custom_def = input('Then insert your defintion: ')\n",
    "              trie_dict.insert(selected_item, custom_def)      \n",
    "            else:\n",
    "              print('Have a good day!')\n",
    "    else:\n",
    "      print(\"Would you like to input it in the system, along with a definition?\")\n",
    "      input_choice  = input('[yes/no]');\n",
    "      if input_choice == 'yes':\n",
    "        word = input('Input word')\n",
    "        definition = input('Input associated definition')\n",
    "        trie_dict.insert(word, definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dictionary(dict):\n",
    " \n",
    "  # __init__ function\n",
    "  def __init__(self):\n",
    "    self = dict()\n",
    " \n",
    "  # Function to add key:value\n",
    "  def add(self, key, value):\n",
    "    self[key] = value\n",
    " \n",
    " \n",
    "# Main Function\n",
    "dict_obj = my_dictionary()\n",
    "glossary_terms_ds = pd.DataFrame(columns = [['Term', 'Definition']])\n",
    "df = pd.DataFrame(list(dict_obj.items()), columns=['Terms', 'Definitions'])\n",
    "df.to_csv('wordnet_english_dictionary.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying another BERT to train it on the English dictionary again but failing miserably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import pandas as pd\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the input and output tokens\n",
    "input_tokens = [tokenizer.encode(input_word, add_special_tokens=False) for input_word in df['Terms']]\n",
    "output_tokens = [tokenizer.encode(output_word, add_special_tokens=False) for output_word in df['Definitions']]\n",
    "\n",
    "# Convert the tokens into PyTorch tensors\n",
    "input_tensors = [torch.tensor(input_token) for input_token in input_tokens]\n",
    "output_tensors = [torch.tensor(output_token) for output_token in output_tokens]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SherlockEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57d0fe2675c1864a6211176a164e0da308406f51aa8b24d41036509a8cf66bc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
